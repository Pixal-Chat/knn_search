{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>image_src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Anthropic CEO goes full techno-optimist in 15,...</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>Anthropic CEO Dario Amodei wants you to know h...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Researchers question AI’s ‘reasoning’ ability ...</td>\n",
       "      <td>Devin Coldewey</td>\n",
       "      <td>How do machine learning models do what they do...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Silicon Valley is debating if AI weapons shoul...</td>\n",
       "      <td>Margaux MacColl</td>\n",
       "      <td>In late September, Shield AI co-founder Brando...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Elon Musk unveils the Robovan: the biggest sur...</td>\n",
       "      <td>Rebecca Bellan</td>\n",
       "      <td>Elon Musk unveiled a prototype of Tesla’s Robo...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tesla reveals 20 Cybercabs at We, Robot event,...</td>\n",
       "      <td>Rebecca Bellan</td>\n",
       "      <td>Tesla has finally revealed its Cybercab, and i...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title           author  \\\n",
       "0   1  Anthropic CEO goes full techno-optimist in 15,...     Kyle Wiggers   \n",
       "1   2  Researchers question AI’s ‘reasoning’ ability ...   Devin Coldewey   \n",
       "2   3  Silicon Valley is debating if AI weapons shoul...  Margaux MacColl   \n",
       "3   4  Elon Musk unveils the Robovan: the biggest sur...   Rebecca Bellan   \n",
       "4   5  Tesla reveals 20 Cybercabs at We, Robot event,...   Rebecca Bellan   \n",
       "\n",
       "                                             content  \\\n",
       "0  Anthropic CEO Dario Amodei wants you to know h...   \n",
       "1  How do machine learning models do what they do...   \n",
       "2  In late September, Shield AI co-founder Brando...   \n",
       "3  Elon Musk unveiled a prototype of Tesla’s Robo...   \n",
       "4  Tesla has finally revealed its Cybercab, and i...   \n",
       "\n",
       "                                           image_src  \n",
       "0  https://techcrunch.com/wp-content/uploads/2023...  \n",
       "1  https://techcrunch.com/wp-content/uploads/2024...  \n",
       "2  https://techcrunch.com/wp-content/uploads/2023...  \n",
       "3  https://techcrunch.com/wp-content/uploads/2024...  \n",
       "4  https://techcrunch.com/wp-content/uploads/2024...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"news_1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>image_src</th>\n",
       "      <th>TitleContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Anthropic CEO goes full techno-optimist in 15,...</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>Anthropic CEO Dario Amodei wants you to know h...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2023...</td>\n",
       "      <td>Anthropic CEO goes full techno-optimist in 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Researchers question AI’s ‘reasoning’ ability ...</td>\n",
       "      <td>Devin Coldewey</td>\n",
       "      <td>How do machine learning models do what they do...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "      <td>Researchers question AI’s ‘reasoning’ ability ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Silicon Valley is debating if AI weapons shoul...</td>\n",
       "      <td>Margaux MacColl</td>\n",
       "      <td>In late September, Shield AI co-founder Brando...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2023...</td>\n",
       "      <td>Silicon Valley is debating if AI weapons shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Elon Musk unveils the Robovan: the biggest sur...</td>\n",
       "      <td>Rebecca Bellan</td>\n",
       "      <td>Elon Musk unveiled a prototype of Tesla’s Robo...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "      <td>Elon Musk unveils the Robovan: the biggest sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tesla reveals 20 Cybercabs at We, Robot event,...</td>\n",
       "      <td>Rebecca Bellan</td>\n",
       "      <td>Tesla has finally revealed its Cybercab, and i...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2024...</td>\n",
       "      <td>Tesla reveals 20 Cybercabs at We, Robot event,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title           author  \\\n",
       "0   1  Anthropic CEO goes full techno-optimist in 15,...     Kyle Wiggers   \n",
       "1   2  Researchers question AI’s ‘reasoning’ ability ...   Devin Coldewey   \n",
       "2   3  Silicon Valley is debating if AI weapons shoul...  Margaux MacColl   \n",
       "3   4  Elon Musk unveils the Robovan: the biggest sur...   Rebecca Bellan   \n",
       "4   5  Tesla reveals 20 Cybercabs at We, Robot event,...   Rebecca Bellan   \n",
       "\n",
       "                                             content  \\\n",
       "0  Anthropic CEO Dario Amodei wants you to know h...   \n",
       "1  How do machine learning models do what they do...   \n",
       "2  In late September, Shield AI co-founder Brando...   \n",
       "3  Elon Musk unveiled a prototype of Tesla’s Robo...   \n",
       "4  Tesla has finally revealed its Cybercab, and i...   \n",
       "\n",
       "                                           image_src  \\\n",
       "0  https://techcrunch.com/wp-content/uploads/2023...   \n",
       "1  https://techcrunch.com/wp-content/uploads/2024...   \n",
       "2  https://techcrunch.com/wp-content/uploads/2023...   \n",
       "3  https://techcrunch.com/wp-content/uploads/2024...   \n",
       "4  https://techcrunch.com/wp-content/uploads/2024...   \n",
       "\n",
       "                                        TitleContent  \n",
       "0  Anthropic CEO goes full techno-optimist in 15,...  \n",
       "1  Researchers question AI’s ‘reasoning’ ability ...  \n",
       "2  Silicon Valley is debating if AI weapons shoul...  \n",
       "3  Elon Musk unveils the Robovan: the biggest sur...  \n",
       "4  Tesla reveals 20 Cybercabs at We, Robot event,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"TitleContent\"] = df['title'] + df[\"content\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = get_embedding(\"magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TitleContentVector\"] = df[\"TitleContent\"].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df.to_csv(\"embedded_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"9vHeVSOenkdmn8Jf2Uav\"),\n",
    "    ca_certs=\"/Users/manishyella/Downloads/elasticsearch-8.15.2/config/certs/http_ca.crt\"\n",
    ")\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexMapping import indexMapping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'news_content_14'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=\"news_content_14\", mappings=indexMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.read_csv(\"embedded_1.csv\", index_col=0)\n",
    "embedding_df[\"TitleContentVector\"] = embedding_df.TitleContentVector.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'title': 'Anthropic CEO goes full techno-optimist in 15,000-word paean to AI',\n",
       "  'author': 'Kyle Wiggers',\n",
       "  'content': 'Anthropic CEO Dario Amodei wants you to know he’s not an AI “doomer.” At least, that’s my read of the “mic drop” of a ~15,000 word essay Amodei published to his blog late Friday. (I tried asking Anthropic’s Claude chatbot whether it concurred, but alas, the post exceeded the free plan’s length limit.) In broad strokes, Amodei paints a picture of a world in which all AI risks are mitigated, and the tech delivers heretofore unrealized prosperity, social uplift, and abundance. He asserts this isn’t to minimize AI’s downsides — at the start, Amodei takes aim, without naming names, at AI companies overselling and generally propagandizing their tech’s capabilities. But one might argue that the essay leans too far in the techno-utopianist direction, making claims simply unsupported by fact. Amodei believes that “powerful AI” will arrive as soon as 2026. By powerful AI, he means AI that’s “smarter than a Nobel Prize winner” in fields like biology and engineering, and that can perform tasks like proving unsolved mathematical theorems and writing “extremely good novels.” This AI, Amodei says, will be able to control any software or hardware imaginable, including industrial machinery, and essentially do most jobs humans do today — but better. “[This AI] can engage in any actions, communications, or remote operations … including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on,” Amodei writes. “It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory it could even design robots or equipment for itself to use.” Lots would have to happen to reach that point. Even the best AI today can’t “think” in the way we understand it. Models don’t so much reason as replicate patterns they’ve observed in their training data. Assuming for the purpose of Amodei’s argument that the AI industry does soon “solve” human-like thought, would robotics catch up to allow future AI to perform lab experiments, manufacture its own tools, and so on? The brittleness of today’s robots imply it’s a long shot. Yet Amodei is optimistic — very optimistic. He believes AI could, in the next 7-12 years, help treat nearly all infectious diseases, eliminate most cancers, cure genetic disorders, and halt Alzheimer’s at the earliest stages. In the next 5-10 years, Amodei thinks that conditions like PTSD, depression, schizophrenia, and addiction will be cured with AI-concocted drugs, or genetically prevented via embryo screening (a controversial opinion) — and that AI-developed drugs will also exist that “tune cognitive function and emotional state” to “get [our brains] to behave a bit better and have a more fulfilling day-to-day experience.” Should this come to pass, Amodei expects the average human lifespan to double to 150. “My basic prediction is that AI-enabled biology and medicine will allow us to compress the progress that human biologists would have achieved over the next 50-100 years into 5-10 years,” he writes. “I’ll refer to this as the ‘compressed 21st century’: the idea that after powerful AI is developed, we will in a few years make all the progress in biology and medicine that we would have made in the whole 21st century.” These seem like stretches, too, considering that AI hasn’t radically transformed medicine yet — and may not for quite some time, or ever. Even if AI does reduce the labor and cost involved in getting a drug into pre-clinical testing, it may fail at a later stage, just like human-designed drugs. Consider that the AI deployed in healthcare today has been shown to be biased and risky in a number of ways, or otherwise incredibly difficult to implement in existing clinical and lab settings. Suggesting all these issues and more will be solved roughly within the decade seems, well, aspirational. But Amodei doesn’t stop there. AI could solve world hunger, he claims. It could turn the tide on climate change. And it could transform the economies in most developing countries; Amodei believes AI can bring the per-capita GDP of sub-Saharan Africa ($1,701 as of 2022) to the per-capita GDP of China ($12,720 in 2022) in 5-10 years. These are bold pronouncements, although likely familiar to anyone who’s listened to disciples of the “Singularity” movement, which expects similar results. To Amodei’s credit, he acknowledges that such developments would require “a huge effort in global health, philanthropy, [and] political advocacy,” which he posits will occur because it’s in the world’s best economic interest. That would be a dramatic change in human behavior if so, given people have shown time and again that their primary interest is in what benefits them in the shorter term. (Deforestation is but one example among thousands.) It’s also worth noting that many of the workers responsible for labeling the datasets used to train AI are paid far below minimum wage while their employers reap tens of millions — or hundreds of millions — in capital from the results. Amodei touches, briefly, on the dangers of AI to civil society, proposing that a coalition of democracies secure AI’s supply chain and block adversaries who intend to use AI toward harmful ends from the means of powerful AI production (semiconductors, etc.). In the same breath, he suggests that AI, in the right hands, could be used to “undermine repressive governments” and even reduce bias in the legal system. (AI has historically exacerbated biases in the legal system.) “A truly mature and successful implementation of AI has the potential to reduce bias and be fairer for everyone,” Amodei writes. So, if AI takes over every conceivable job and does it better and faster, won’t that leave humans in a lurch economically speaking? Amodei admits that, yes, it would, and that at that point, society would have to have conversations about “how the economy should be organized.” But he offers no solution. “People do want a sense of accomplishment, even a sense of competition, and in a post-AI world it will be perfectly possible to spend years attempting some very difficult task with a complex strategy, similar to what people do today when they embark on research projects, try to become Hollywood actors, or found companies,” he writes. “The facts that (a) an AI somewhere could in principle do this task better, and (b) this task is no longer an economically rewarded element of a global economy, don’t seem to me to matter very much.” Amodei advances the notion, in wrapping up, that AI is simply a technological accelerator — that humans naturally trend toward “rule of law, democracy, and Enlightenment values.” But in doing so, he ignores AI’s many costs. AI is projected to have — is already having — an enormous environmental impact. And it’s creating inequality. Nobel Prize-winning economist Joseph Stiglitz and others have noted the labor disruptions caused by AI could further concentrate wealth in the hands of companies and leave workers more powerless than ever. These companies include Anthropic, as loath as Amodei is to admit it. Anthropic is a business, after all — one reportedly worth close to $40 billion. And those benefiting from its AI tech are, by and large, corporations whose only responsibility is to boost returns to shareholders, not better humanity. A cynic might question the essay’s timing, in fact, given that Anthropic is said to be in the process of raising billions of dollars in venture funds. OpenAI CEO Sam Altman published a similarly technopotimist manifesto shortly before OpenAI closed a $6.5 billion funding round. Perhaps it’s a coincidence. Then again, Amodei isn’t a philanthropist. Like any CEO, he has a product to pitch. It just so happens that his product is going to “save the world” — and those who think otherwise risk being left behind. Or so he’d have you believe.             ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2023/09/53202070940_ea57312b1a_k.jpg?w=1024',\n",
       "  'TitleContent': 'Anthropic CEO goes full techno-optimist in 15,000-word paean to AIAnthropic CEO Dario Amodei wants you to know he’s not an AI “doomer.” At least, that’s my read of the “mic drop” of a ~15,000 word essay Amodei published to his blog late Friday. (I tried asking Anthropic’s Claude chatbot whether it concurred, but alas, the post exceeded the free plan’s length limit.) In broad strokes, Amodei paints a picture of a world in which all AI risks are mitigated, and the tech delivers heretofore unrealized prosperity, social uplift, and abundance. He asserts this isn’t to minimize AI’s downsides — at the start, Amodei takes aim, without naming names, at AI companies overselling and generally propagandizing their tech’s capabilities. But one might argue that the essay leans too far in the techno-utopianist direction, making claims simply unsupported by fact. Amodei believes that “powerful AI” will arrive as soon as 2026. By powerful AI, he means AI that’s “smarter than a Nobel Prize winner” in fields like biology and engineering, and that can perform tasks like proving unsolved mathematical theorems and writing “extremely good novels.” This AI, Amodei says, will be able to control any software or hardware imaginable, including industrial machinery, and essentially do most jobs humans do today — but better. “[This AI] can engage in any actions, communications, or remote operations … including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on,” Amodei writes. “It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory it could even design robots or equipment for itself to use.” Lots would have to happen to reach that point. Even the best AI today can’t “think” in the way we understand it. Models don’t so much reason as replicate patterns they’ve observed in their training data. Assuming for the purpose of Amodei’s argument that the AI industry does soon “solve” human-like thought, would robotics catch up to allow future AI to perform lab experiments, manufacture its own tools, and so on? The brittleness of today’s robots imply it’s a long shot. Yet Amodei is optimistic — very optimistic. He believes AI could, in the next 7-12 years, help treat nearly all infectious diseases, eliminate most cancers, cure genetic disorders, and halt Alzheimer’s at the earliest stages. In the next 5-10 years, Amodei thinks that conditions like PTSD, depression, schizophrenia, and addiction will be cured with AI-concocted drugs, or genetically prevented via embryo screening (a controversial opinion) — and that AI-developed drugs will also exist that “tune cognitive function and emotional state” to “get [our brains] to behave a bit better and have a more fulfilling day-to-day experience.” Should this come to pass, Amodei expects the average human lifespan to double to 150. “My basic prediction is that AI-enabled biology and medicine will allow us to compress the progress that human biologists would have achieved over the next 50-100 years into 5-10 years,” he writes. “I’ll refer to this as the ‘compressed 21st century’: the idea that after powerful AI is developed, we will in a few years make all the progress in biology and medicine that we would have made in the whole 21st century.” These seem like stretches, too, considering that AI hasn’t radically transformed medicine yet — and may not for quite some time, or ever. Even if AI does reduce the labor and cost involved in getting a drug into pre-clinical testing, it may fail at a later stage, just like human-designed drugs. Consider that the AI deployed in healthcare today has been shown to be biased and risky in a number of ways, or otherwise incredibly difficult to implement in existing clinical and lab settings. Suggesting all these issues and more will be solved roughly within the decade seems, well, aspirational. But Amodei doesn’t stop there. AI could solve world hunger, he claims. It could turn the tide on climate change. And it could transform the economies in most developing countries; Amodei believes AI can bring the per-capita GDP of sub-Saharan Africa ($1,701 as of 2022) to the per-capita GDP of China ($12,720 in 2022) in 5-10 years. These are bold pronouncements, although likely familiar to anyone who’s listened to disciples of the “Singularity” movement, which expects similar results. To Amodei’s credit, he acknowledges that such developments would require “a huge effort in global health, philanthropy, [and] political advocacy,” which he posits will occur because it’s in the world’s best economic interest. That would be a dramatic change in human behavior if so, given people have shown time and again that their primary interest is in what benefits them in the shorter term. (Deforestation is but one example among thousands.) It’s also worth noting that many of the workers responsible for labeling the datasets used to train AI are paid far below minimum wage while their employers reap tens of millions — or hundreds of millions — in capital from the results. Amodei touches, briefly, on the dangers of AI to civil society, proposing that a coalition of democracies secure AI’s supply chain and block adversaries who intend to use AI toward harmful ends from the means of powerful AI production (semiconductors, etc.). In the same breath, he suggests that AI, in the right hands, could be used to “undermine repressive governments” and even reduce bias in the legal system. (AI has historically exacerbated biases in the legal system.) “A truly mature and successful implementation of AI has the potential to reduce bias and be fairer for everyone,” Amodei writes. So, if AI takes over every conceivable job and does it better and faster, won’t that leave humans in a lurch economically speaking? Amodei admits that, yes, it would, and that at that point, society would have to have conversations about “how the economy should be organized.” But he offers no solution. “People do want a sense of accomplishment, even a sense of competition, and in a post-AI world it will be perfectly possible to spend years attempting some very difficult task with a complex strategy, similar to what people do today when they embark on research projects, try to become Hollywood actors, or found companies,” he writes. “The facts that (a) an AI somewhere could in principle do this task better, and (b) this task is no longer an economically rewarded element of a global economy, don’t seem to me to matter very much.” Amodei advances the notion, in wrapping up, that AI is simply a technological accelerator — that humans naturally trend toward “rule of law, democracy, and Enlightenment values.” But in doing so, he ignores AI’s many costs. AI is projected to have — is already having — an enormous environmental impact. And it’s creating inequality. Nobel Prize-winning economist Joseph Stiglitz and others have noted the labor disruptions caused by AI could further concentrate wealth in the hands of companies and leave workers more powerless than ever. These companies include Anthropic, as loath as Amodei is to admit it. Anthropic is a business, after all — one reportedly worth close to $40 billion. And those benefiting from its AI tech are, by and large, corporations whose only responsibility is to boost returns to shareholders, not better humanity. A cynic might question the essay’s timing, in fact, given that Anthropic is said to be in the process of raising billions of dollars in venture funds. OpenAI CEO Sam Altman published a similarly technopotimist manifesto shortly before OpenAI closed a $6.5 billion funding round. Perhaps it’s a coincidence. Then again, Amodei isn’t a philanthropist. Like any CEO, he has a product to pitch. It just so happens that his product is going to “save the world” — and those who think otherwise risk being left behind. Or so he’d have you believe.             ',\n",
       "  'TitleContentVector': array([-0.00272038, -0.02569323, -0.0004421 , ..., -0.03079831,\n",
       "         -0.00357181, -0.0192874 ])},\n",
       " {'id': 2,\n",
       "  'title': 'Researchers question AI’s ‘reasoning’ ability as models stumble on math problems with trivial changes',\n",
       "  'author': 'Devin Coldewey',\n",
       "  'content': 'How do machine learning models do what they do? And are they really “thinking” or “reasoning” the way we understand those things? This is a philosophical question as much as a practical one, but a new paper making the rounds Friday suggests that the answer is, at least for now, a pretty clear “no.” A group of AI research scientists at Apple released their paper, “Understanding the limitations of mathematical reasoning in large language models,” to general commentary Thursday. While the deeper concepts of symbolic learning and pattern reproduction are a bit in the weeds, the basic concept of their research is very easy to grasp. Let’s say I asked you to solve a simple math problem like this one: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday. How many kiwis does Oliver have? Obviously, the answer is 44 + 58 + (44 * 2) = 190. Though large language models are actually spotty on arithmetic, they can pretty reliably solve something like this. But what if I threw in a little random extra info, like this: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday, but five of them were a bit smaller than average. How many kiwis does Oliver have? It’s the same math problem, right? And of course even a grade-schooler would know that even a small kiwi is still a kiwi. But as it turns out, this extra data point confuses even state-of-the-art LLMs. Here’s GPT-o1-mini’s take: … on Sunday, 5 of these kiwis were smaller than average. We need to subtract them from the Sunday total: 88 (Sunday’s kiwis) – 5 (smaller kiwis) = 83 kiwis This is just a simple example out of hundreds of questions that the researchers lightly modified, but nearly all of which led to enormous drops in success rates for the models attempting them. Now, why should this be? Why would a model that understands the problem be thrown off so easily by a random, irrelevant detail? The researchers propose that this reliable mode of failure means the models don’t really understand the problem at all. Their training data does allow them to respond with the correct answer in some situations, but as soon as the slightest actual “reasoning” is required, such as whether to count small kiwis, they start producing weird, unintuitive results. As the researchers put it in their paper: [W]e investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. This observation is consistent with the other qualities often attributed to LLMs due to their facility with language. When, statistically, the phrase “I love you” is followed by “I love you, too,” the LLM can easily repeat that — but it doesn’t mean it loves you. And although it can follow complex chains of reasoning it has been exposed to before, the fact that this chain can be broken by even superficial deviations suggests that it doesn’t actually reason so much as replicate patterns it has observed in its training data. Mehrdad Farajtabar, one of the co-authors, breaks down the paper very nicely in this thread on X. An OpenAI researcher, while commending Mirzadeh et al’s work, objected to their conclusions, saying that correct results could likely be achieved in all these failure cases with a bit of prompt engineering. Farajtabar (responding with the typical yet admirable friendliness researchers tend to employ) noted that while better prompting may work for simple deviations, the model may require exponentially more contextual data in order to counter complex distractions — ones that, again, a child could trivially point out. Does this mean that LLMs don’t reason? Maybe. That they can’t reason? No one knows. These are not well-defined concepts, and the questions tend to appear at the bleeding edge of AI research, where the state of the art changes on a daily basis. Perhaps LLMs “reason,” but in a way we don’t yet recognize or know how to control. It makes for a fascinating frontier in research, but it’s also a cautionary tale when it comes to how AI is being sold. Can it really do the things they claim, and if it does, how? As AI becomes an everyday software tool, this kind of question is no longer academic.         ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-496822526.jpg?w=1024',\n",
       "  'TitleContent': 'Researchers question AI’s ‘reasoning’ ability as models stumble on math problems with trivial changesHow do machine learning models do what they do? And are they really “thinking” or “reasoning” the way we understand those things? This is a philosophical question as much as a practical one, but a new paper making the rounds Friday suggests that the answer is, at least for now, a pretty clear “no.” A group of AI research scientists at Apple released their paper, “Understanding the limitations of mathematical reasoning in large language models,” to general commentary Thursday. While the deeper concepts of symbolic learning and pattern reproduction are a bit in the weeds, the basic concept of their research is very easy to grasp. Let’s say I asked you to solve a simple math problem like this one: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday. How many kiwis does Oliver have? Obviously, the answer is 44 + 58 + (44 * 2) = 190. Though large language models are actually spotty on arithmetic, they can pretty reliably solve something like this. But what if I threw in a little random extra info, like this: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday, but five of them were a bit smaller than average. How many kiwis does Oliver have? It’s the same math problem, right? And of course even a grade-schooler would know that even a small kiwi is still a kiwi. But as it turns out, this extra data point confuses even state-of-the-art LLMs. Here’s GPT-o1-mini’s take: … on Sunday, 5 of these kiwis were smaller than average. We need to subtract them from the Sunday total: 88 (Sunday’s kiwis) – 5 (smaller kiwis) = 83 kiwis This is just a simple example out of hundreds of questions that the researchers lightly modified, but nearly all of which led to enormous drops in success rates for the models attempting them. Now, why should this be? Why would a model that understands the problem be thrown off so easily by a random, irrelevant detail? The researchers propose that this reliable mode of failure means the models don’t really understand the problem at all. Their training data does allow them to respond with the correct answer in some situations, but as soon as the slightest actual “reasoning” is required, such as whether to count small kiwis, they start producing weird, unintuitive results. As the researchers put it in their paper: [W]e investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. This observation is consistent with the other qualities often attributed to LLMs due to their facility with language. When, statistically, the phrase “I love you” is followed by “I love you, too,” the LLM can easily repeat that — but it doesn’t mean it loves you. And although it can follow complex chains of reasoning it has been exposed to before, the fact that this chain can be broken by even superficial deviations suggests that it doesn’t actually reason so much as replicate patterns it has observed in its training data. Mehrdad Farajtabar, one of the co-authors, breaks down the paper very nicely in this thread on X. An OpenAI researcher, while commending Mirzadeh et al’s work, objected to their conclusions, saying that correct results could likely be achieved in all these failure cases with a bit of prompt engineering. Farajtabar (responding with the typical yet admirable friendliness researchers tend to employ) noted that while better prompting may work for simple deviations, the model may require exponentially more contextual data in order to counter complex distractions — ones that, again, a child could trivially point out. Does this mean that LLMs don’t reason? Maybe. That they can’t reason? No one knows. These are not well-defined concepts, and the questions tend to appear at the bleeding edge of AI research, where the state of the art changes on a daily basis. Perhaps LLMs “reason,” but in a way we don’t yet recognize or know how to control. It makes for a fascinating frontier in research, but it’s also a cautionary tale when it comes to how AI is being sold. Can it really do the things they claim, and if it does, how? As AI becomes an everyday software tool, this kind of question is no longer academic.         ',\n",
       "  'TitleContentVector': array([ 0.00839916,  0.01272639,  0.00019834, ..., -0.0188932 ,\n",
       "         -0.02814995, -0.03186836])},\n",
       " {'id': 3,\n",
       "  'title': 'Silicon Valley is debating if AI weapons should be allowed to decide to kill',\n",
       "  'author': 'Margaux MacColl',\n",
       "  'content': 'In late September, Shield AI co-founder Brandon Tseng swore that weapons in the U.S. would never be fully autonomous — meaning an AI algorithm would make the final decision to kill someone. “Congress doesn’t want that,” the defense tech founder told TechCrunch. “No one wants that.” But Tseng spoke too soon. Five days later, Anduril co-founder Palmer Luckey expressed an openness to autonomous weapons — or at least a heavy skepticism of arguments against them. The U.S.’s adversaries “use phrases that sound really good in a sound bite: Well, can’t you agree that a robot should never be able to decide who lives and dies?” Luckey said during a talk earlier this month at Pepperdine University. “And my point to them is, where’s the moral high ground in a landmine that can’t tell the difference between a school bus full of kids and a Russian tank?” When asked for further comment, Shannon Prior, a spokesperson for Anduril, said that Luckey didn’t mean that robots should be programmed to kill people on their own, just that he was concerned about “bad people using bad AI.” In the past, Silicon Valley has erred on the side of caution. Take it from Luckey’s co-founder, Trae Stephens. “I think the technologies that we’re building are making it possible for humans to make the right decisions about these things,” he told Kara Swisher last year. “So that there is an accountable, responsible party in the loop for all decisions that could involve lethality, obviously.” The Anduril spokesperson denied any dissonance between Luckey (pictured above) and Stephens’ perspectives, and said that Stephens didn’t mean that a human should always make the call, but just that someone is accountable. To be fair, the stance of the U.S. government itself is similarly ambiguous. The U.S. military currently does not purchase fully autonomous weapons. Though some argue weapons like mines and missiles can operate autonomously, this is a qualitatively different form of autonomy than, say, a turret that identifies, acquires, and fires on targets without human intervention. The U.S. does not ban companies from making fully autonomous lethal weapons nor does it explicitly ban them from selling such things to foreign countries. Last year, the U.S. released updated guidelines for AI safety in the military that have been endorsed by many U.S. allies and requires top military officials to approve of any new autonomous weapon; yet the guidelines are voluntary (Anduril said it is committed to following the guidelines), and U.S. officials have continuously said it’s “not the right time” to consider any binding ban on autonomous weapons. Last month, Palantir co-founder and Anduril investor Joe Lonsdale also showed a willingness to consider fully autonomous weapons. At an event hosted by the think tank Hudson Institute, Lonsdale expressed frustration that this question is being framed as a yes-or-no at all. He instead presented a hypothetical where China has embraced AI weapons, but the U.S. has to “press the button every time it fires.” He encouraged policymakers to embrace a more flexible approach to how much AI is in weapons. “You very quickly realize, well, my assumptions were wrong if I just put a stupid top-down rule, because I’m a staffer who’s never played this game before,” he said. “I could destroy us in the battle.” When TechCrunch asked Lonsdale for further comment, he emphasized that defense tech companies shouldn’t be the ones setting the agenda on lethal AI. “The key context to what I was saying is that our companies don’t make the policy, and don’t want to make the policy: it’s the job of elected officials to make the policy,” he said. “But they do need to educate themselves on the nuance to do a good job.” He also reiterated a willingness to consider more autonomy in weapons. “It’s not a binary as you suggest — ‘fully autonomous or not’ isn’t the correct policy question. There’s a sophisticated dial along a few different dimensions for what you might have a soldier do and what you have the weapons system do,” he said. “Before policymakers put these rules in place and decide where the dials need to be set in what circumstance, they need to learn the game and learn what the bad guys might be doing, and what’s necessary to win with American lives on the line.” Activists and human rights groups have long tried and failed to establish international bans on autonomous lethal weapons — bans that the U.S. has resisted signing. But the war in Ukraine may have turned the tide against activists, providing both a trove of combat data and a battlefield for defense tech founders to test on. Currently, companies integrate AI into weapons systems, although they still require a human to make the final decision to kill. Meanwhile, Ukrainian officials have pushed for more automation in weapons, hoping it’ll give them a leg-up over Russia. “We need maximum automation,” said Mykhailo Fedorov, Ukraine’s minister of digital transformation, in an interview with The New York Times. “These technologies are fundamental to our victory.” For many in Silicon Valley and D.C., the biggest fear is that China or Russia rolls out fully autonomous weapons first, forcing the U.S.’s hand. At a UN debate on AI arms last year, a Russian diplomat was notably coy. “We understand that for many delegations the priority is human control,” he said. “For the Russian Federation, the priorities are somewhat different.” At the Hudson Institute event, Lonsdale said that the tech sector needs to take it upon itself to “teach the Navy, teach the DoD, teach Congress” about the potential of AI to “hopefully get us ahead of China.” Lonsdale’s and Luckey’s affiliated companies are working on getting Congress to listen to them. Anduril and Palantir have cumulatively spent over $4 million in lobbying this year, according to OpenSecrets. Editor’s note: this story was updated with more language to describe autonomous weapons.         ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2023/12/GettyImages-1258953220-1.jpg?w=1024',\n",
       "  'TitleContent': 'Silicon Valley is debating if AI weapons should be allowed to decide to killIn late September, Shield AI co-founder Brandon Tseng swore that weapons in the U.S. would never be fully autonomous — meaning an AI algorithm would make the final decision to kill someone. “Congress doesn’t want that,” the defense tech founder told TechCrunch. “No one wants that.” But Tseng spoke too soon. Five days later, Anduril co-founder Palmer Luckey expressed an openness to autonomous weapons — or at least a heavy skepticism of arguments against them. The U.S.’s adversaries “use phrases that sound really good in a sound bite: Well, can’t you agree that a robot should never be able to decide who lives and dies?” Luckey said during a talk earlier this month at Pepperdine University. “And my point to them is, where’s the moral high ground in a landmine that can’t tell the difference between a school bus full of kids and a Russian tank?” When asked for further comment, Shannon Prior, a spokesperson for Anduril, said that Luckey didn’t mean that robots should be programmed to kill people on their own, just that he was concerned about “bad people using bad AI.” In the past, Silicon Valley has erred on the side of caution. Take it from Luckey’s co-founder, Trae Stephens. “I think the technologies that we’re building are making it possible for humans to make the right decisions about these things,” he told Kara Swisher last year. “So that there is an accountable, responsible party in the loop for all decisions that could involve lethality, obviously.” The Anduril spokesperson denied any dissonance between Luckey (pictured above) and Stephens’ perspectives, and said that Stephens didn’t mean that a human should always make the call, but just that someone is accountable. To be fair, the stance of the U.S. government itself is similarly ambiguous. The U.S. military currently does not purchase fully autonomous weapons. Though some argue weapons like mines and missiles can operate autonomously, this is a qualitatively different form of autonomy than, say, a turret that identifies, acquires, and fires on targets without human intervention. The U.S. does not ban companies from making fully autonomous lethal weapons nor does it explicitly ban them from selling such things to foreign countries. Last year, the U.S. released updated guidelines for AI safety in the military that have been endorsed by many U.S. allies and requires top military officials to approve of any new autonomous weapon; yet the guidelines are voluntary (Anduril said it is committed to following the guidelines), and U.S. officials have continuously said it’s “not the right time” to consider any binding ban on autonomous weapons. Last month, Palantir co-founder and Anduril investor Joe Lonsdale also showed a willingness to consider fully autonomous weapons. At an event hosted by the think tank Hudson Institute, Lonsdale expressed frustration that this question is being framed as a yes-or-no at all. He instead presented a hypothetical where China has embraced AI weapons, but the U.S. has to “press the button every time it fires.” He encouraged policymakers to embrace a more flexible approach to how much AI is in weapons. “You very quickly realize, well, my assumptions were wrong if I just put a stupid top-down rule, because I’m a staffer who’s never played this game before,” he said. “I could destroy us in the battle.” When TechCrunch asked Lonsdale for further comment, he emphasized that defense tech companies shouldn’t be the ones setting the agenda on lethal AI. “The key context to what I was saying is that our companies don’t make the policy, and don’t want to make the policy: it’s the job of elected officials to make the policy,” he said. “But they do need to educate themselves on the nuance to do a good job.” He also reiterated a willingness to consider more autonomy in weapons. “It’s not a binary as you suggest — ‘fully autonomous or not’ isn’t the correct policy question. There’s a sophisticated dial along a few different dimensions for what you might have a soldier do and what you have the weapons system do,” he said. “Before policymakers put these rules in place and decide where the dials need to be set in what circumstance, they need to learn the game and learn what the bad guys might be doing, and what’s necessary to win with American lives on the line.” Activists and human rights groups have long tried and failed to establish international bans on autonomous lethal weapons — bans that the U.S. has resisted signing. But the war in Ukraine may have turned the tide against activists, providing both a trove of combat data and a battlefield for defense tech founders to test on. Currently, companies integrate AI into weapons systems, although they still require a human to make the final decision to kill. Meanwhile, Ukrainian officials have pushed for more automation in weapons, hoping it’ll give them a leg-up over Russia. “We need maximum automation,” said Mykhailo Fedorov, Ukraine’s minister of digital transformation, in an interview with The New York Times. “These technologies are fundamental to our victory.” For many in Silicon Valley and D.C., the biggest fear is that China or Russia rolls out fully autonomous weapons first, forcing the U.S.’s hand. At a UN debate on AI arms last year, a Russian diplomat was notably coy. “We understand that for many delegations the priority is human control,” he said. “For the Russian Federation, the priorities are somewhat different.” At the Hudson Institute event, Lonsdale said that the tech sector needs to take it upon itself to “teach the Navy, teach the DoD, teach Congress” about the potential of AI to “hopefully get us ahead of China.” Lonsdale’s and Luckey’s affiliated companies are working on getting Congress to listen to them. Anduril and Palantir have cumulatively spent over $4 million in lobbying this year, according to OpenSecrets. Editor’s note: this story was updated with more language to describe autonomous weapons.         ',\n",
       "  'TitleContentVector': array([ 0.00647319, -0.01963855, -0.00698298, ..., -0.02368943,\n",
       "         -0.01287112,  0.0111057 ])},\n",
       " {'id': 4,\n",
       "  'title': 'Elon Musk unveils the Robovan: the biggest surprise from Tesla’s We, Robot event',\n",
       "  'author': 'Rebecca Bellan',\n",
       "  'content': 'Elon Musk unveiled a prototype of Tesla’s Robovan on Thursday night during the company’s We, Robot event in Los Angeles. The Robovan will be an electric, autonomous vehicle roughly the size of a bus, designed for transporting people around high density areas. It will carry up to 20 people at a time and also transport goods, according to Musk. “We’re going to make this, and it’s going to look like that,” said Musk on Thursday night as the Robovan rolled towards center stage. That’s about as much as Musk was willing to say, and we’re not even sure that much is true. Musk didn’t mention how much the Robovan would cost, how Tesla would produce it, or when it will come out. However, it does look pretty cool. The Robovan has a retro-futuristic look – somewhere between a bus from The Jetsons and a toaster from the 1950s. It features silver metallic sides with black details, and strips of light running parallel to the ground along its sides, with doors that slide out from the middle. Inside, there are seats and room to stand, with tinted windows throughout. There is no steering wheel, since it’s autonomous. “One of the things we want to do – and we’ve done this with the Cybertruck – is we want to change the look of the roads,” said Musk. “The future should look like the future,” he said, repeating an old line. It looks similar to other purpose-built robotaxis, like those designed by Zoox and Cruise. Only Tesla’s van is much bigger. In China, WeRide has built a similar Robobus. That said, the Robovan showed on Thursday is only a prototype. Despite what Musk says, there’s no telling what the real thing will look like or when it will actually come out. Tesla had kept the design of the vehicles it introduced on Thursday pretty close to the chest. The only real hint we had was from Tesla’s 2023 Investor Day, when the automaker teased a couple of new vehicles that appeared to be designed for volume production: One smaller vehicle that appears now to be the Cybercab, and a larger one that we can now say is likely the Robovan. The stated goal at the time was to produce 20 million vehicles per year by 2030. That would mean that Tesla needs to increase production and sales by about 15 times from 2022. During Thursday’s event, Musk did not outline any plans for building new production facilities or retooling existing facilities to accommodate either the Cybercab or the Robovan. He also didn’t provide much in the way of timelines for the Robovan, though he predicted the Cybercab would start production in 2026 or 2027.     ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2024/10/Screenshot-2024-10-10-at-9.09.45PM.png?w=1024',\n",
       "  'TitleContent': 'Elon Musk unveils the Robovan: the biggest surprise from Tesla’s We, Robot eventElon Musk unveiled a prototype of Tesla’s Robovan on Thursday night during the company’s We, Robot event in Los Angeles. The Robovan will be an electric, autonomous vehicle roughly the size of a bus, designed for transporting people around high density areas. It will carry up to 20 people at a time and also transport goods, according to Musk. “We’re going to make this, and it’s going to look like that,” said Musk on Thursday night as the Robovan rolled towards center stage. That’s about as much as Musk was willing to say, and we’re not even sure that much is true. Musk didn’t mention how much the Robovan would cost, how Tesla would produce it, or when it will come out. However, it does look pretty cool. The Robovan has a retro-futuristic look – somewhere between a bus from The Jetsons and a toaster from the 1950s. It features silver metallic sides with black details, and strips of light running parallel to the ground along its sides, with doors that slide out from the middle. Inside, there are seats and room to stand, with tinted windows throughout. There is no steering wheel, since it’s autonomous. “One of the things we want to do – and we’ve done this with the Cybertruck – is we want to change the look of the roads,” said Musk. “The future should look like the future,” he said, repeating an old line. It looks similar to other purpose-built robotaxis, like those designed by Zoox and Cruise. Only Tesla’s van is much bigger. In China, WeRide has built a similar Robobus. That said, the Robovan showed on Thursday is only a prototype. Despite what Musk says, there’s no telling what the real thing will look like or when it will actually come out. Tesla had kept the design of the vehicles it introduced on Thursday pretty close to the chest. The only real hint we had was from Tesla’s 2023 Investor Day, when the automaker teased a couple of new vehicles that appeared to be designed for volume production: One smaller vehicle that appears now to be the Cybercab, and a larger one that we can now say is likely the Robovan. The stated goal at the time was to produce 20 million vehicles per year by 2030. That would mean that Tesla needs to increase production and sales by about 15 times from 2022. During Thursday’s event, Musk did not outline any plans for building new production facilities or retooling existing facilities to accommodate either the Cybercab or the Robovan. He also didn’t provide much in the way of timelines for the Robovan, though he predicted the Cybercab would start production in 2026 or 2027.     ',\n",
       "  'TitleContentVector': array([ 0.00921395, -0.03970509,  0.01619862, ..., -0.01160154,\n",
       "         -0.00838951, -0.01721433])},\n",
       " {'id': 5,\n",
       "  'title': 'Tesla reveals 20 Cybercabs at We, Robot event, says you’ll be able to buy one for less than $30,000',\n",
       "  'author': 'Rebecca Bellan',\n",
       "  'content': 'Tesla has finally revealed its Cybercab, and it looks like a smaller, sleeker, two-seater Cybertruck. And while many were expecting there to be at least one prototype of a robotaxi with no steering wheel or pedals, Tesla CEO Elon Musk delighted his fans with a lineup of 20 vehicles. The flashy “We, Robot” event took place at Warner Bros. Discovery studio on Thursday. Before walking on stage, Musk walked over to a robotaxi, which opened its gullwing doors, and did a short demo around the well-maintained streets of the Hollywood studio. Musk repeated previous claims that the cost of autonomous transport will be so low, it will be akin to “individualized mass transit.” He said he believed the average operating cost of the Cybercab will be over time around $0.20 per mile. “And you will be able to buy one,” Musk said, adding that the cost of the vehicle would be below $30,000. Musk also noted he expects Tesla to start doing “unsupervised FSD in Texas and California next year” with the Model 3 and Model Y. He acknowledged that he’s too optimistic about timelines, but said he expects the Cybercab to be in production by 2026 or “before 2027.” Fans cheered when Musk said they would be able to test out the Cybercabs themselves at the event. “They have like 20 of them driving around the entire lot totally unsupervised,” one event-goer told TechCrunch. The robotaxi also doesn’t have a plug in charger, and instead has “inductive charging,” which is a sort of wireless charging, according to Musk. Tesla unveiled a surprise Robovan at the event, a sleek-looking autonomous bus that can carry up to 20 people and also transport goods. No timelines were revealed for this vehicle — only a hand-waving of a future that could “change the look of the roads.” Musk also introduced roughly a dozen Optimus humanoid robots, sharing his vision for a future where robots act as friend and helper for the low price of around $20,000 to $30,000. Those bots were walking among the humans at the event, dancing like go-go dancers, and even mixing drinks. They also appeared to be speaking to the guests, and could do different accents and voice personas on command. Though it’s not clear whether those capabilities, and the bots’ movements, were being remote controlled by a human. Tesla was originally slated to reveal its Robotaxi or Cybercab in August, but delayed the unveiling after the executive requested an “important design change to the front.” The Robotaxi unveiling is part of Tesla’s push to go “balls to the wall for autonomy” this year after pivoting from prioritizing the production of a $25,000 EV and laying off 10% of staff, including most of the charging team. But Musk’s vision of an autonomous driving future has been in play for years, and a large part of the reason why investors price Tesla’s stock not as an automaker, but as a technology company. The Cybercab prototype represents one half of the business concept Musk has set forth since at least 2019, wherein the automaker would run its own fleet of dedicated robotaxis on a Tesla ride-hail app, which Tesla teased during its first quarter investor call. Musk has described the other half of the strategy as similar to Uber or Airbnb, where Tesla owners will be able to add their properly equipped vehicles to Tesla’s ride-hailing app to make extra money when the cars are not in use, and Tesla will take 25% to 30% of the revenue (similar to Apple’s App Store take rate). Musk also said that Tesla projected that robotaxi rides would cost less than public transportation, but he didn’t say by when. “By the middle of next year, we’ll have over a million Tesla cars on the road with Full Self-Driving hardware, feature complete, at a reliability level that we would consider that no one needs to pay attention, meaning you could go to sleep,” Musk said at Tesla’s 2019 Autonomy Day. “From our standpoint, if you fast forward a year, maybe a year and three months, but next year for sure, we will have over a million robotaxis on the road. The fleet wakes up with an over the air update. That’s all it takes.” That, of course, didn’t happen by 2020. Tesla’s Full Self-Driving software, which is on hundreds of thousands of vehicles today, relies only on cameras to perceive the environment around it. Industry experts say this vision-only approach is the reason why the software is still not actually fully self-driving, despite its name. FSD can perform many automated driving tasks, but still requires a human behind the wheel to stay attentive and take over if needed. It’s also not clear that existing Teslas even have the right hardware to get to this full self-driving future that Musk has been promising for years. As Musk posted on X in July, the roughly 5x increase in parameter count needed to power Tesla’s next-gen AI “is very difficult to achieve without upgrading the vehicle inference computer.” Regardless, if Tesla wants to commercialize Level 4 autonomous driving – which means the vehicle can drive itself under certain conditions without needing a human to take over – it will need to prove the safety case. Tesla has been under numerous federal investigations for fatal crashes that happened while Autopilot, Tesla’s lower level advanced driver assistance system, was in place. California has the most rigorous permitting process for testing and deployment of autonomous vehicles, but in most other states, Tesla would have to show at a minimum that its vehicles are capable of pulling themselves over safely. Then there’s the matter of the Cybercab’s lack of steering wheels or pedals, which would put it out of compliance with federal vehicle safety laws. GM’s Cruise had previously tried to bring its purpose-built robotaxi, the Origin, to production, but failed to gain the necessary approvals from the National Highway Traffic and Safety Administration before scrapping the project.              ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2024/10/cybercab.png?w=1024',\n",
       "  'TitleContent': 'Tesla reveals 20 Cybercabs at We, Robot event, says you’ll be able to buy one for less than $30,000Tesla has finally revealed its Cybercab, and it looks like a smaller, sleeker, two-seater Cybertruck. And while many were expecting there to be at least one prototype of a robotaxi with no steering wheel or pedals, Tesla CEO Elon Musk delighted his fans with a lineup of 20 vehicles. The flashy “We, Robot” event took place at Warner Bros. Discovery studio on Thursday. Before walking on stage, Musk walked over to a robotaxi, which opened its gullwing doors, and did a short demo around the well-maintained streets of the Hollywood studio. Musk repeated previous claims that the cost of autonomous transport will be so low, it will be akin to “individualized mass transit.” He said he believed the average operating cost of the Cybercab will be over time around $0.20 per mile. “And you will be able to buy one,” Musk said, adding that the cost of the vehicle would be below $30,000. Musk also noted he expects Tesla to start doing “unsupervised FSD in Texas and California next year” with the Model 3 and Model Y. He acknowledged that he’s too optimistic about timelines, but said he expects the Cybercab to be in production by 2026 or “before 2027.” Fans cheered when Musk said they would be able to test out the Cybercabs themselves at the event. “They have like 20 of them driving around the entire lot totally unsupervised,” one event-goer told TechCrunch. The robotaxi also doesn’t have a plug in charger, and instead has “inductive charging,” which is a sort of wireless charging, according to Musk. Tesla unveiled a surprise Robovan at the event, a sleek-looking autonomous bus that can carry up to 20 people and also transport goods. No timelines were revealed for this vehicle — only a hand-waving of a future that could “change the look of the roads.” Musk also introduced roughly a dozen Optimus humanoid robots, sharing his vision for a future where robots act as friend and helper for the low price of around $20,000 to $30,000. Those bots were walking among the humans at the event, dancing like go-go dancers, and even mixing drinks. They also appeared to be speaking to the guests, and could do different accents and voice personas on command. Though it’s not clear whether those capabilities, and the bots’ movements, were being remote controlled by a human. Tesla was originally slated to reveal its Robotaxi or Cybercab in August, but delayed the unveiling after the executive requested an “important design change to the front.” The Robotaxi unveiling is part of Tesla’s push to go “balls to the wall for autonomy” this year after pivoting from prioritizing the production of a $25,000 EV and laying off 10% of staff, including most of the charging team. But Musk’s vision of an autonomous driving future has been in play for years, and a large part of the reason why investors price Tesla’s stock not as an automaker, but as a technology company. The Cybercab prototype represents one half of the business concept Musk has set forth since at least 2019, wherein the automaker would run its own fleet of dedicated robotaxis on a Tesla ride-hail app, which Tesla teased during its first quarter investor call. Musk has described the other half of the strategy as similar to Uber or Airbnb, where Tesla owners will be able to add their properly equipped vehicles to Tesla’s ride-hailing app to make extra money when the cars are not in use, and Tesla will take 25% to 30% of the revenue (similar to Apple’s App Store take rate). Musk also said that Tesla projected that robotaxi rides would cost less than public transportation, but he didn’t say by when. “By the middle of next year, we’ll have over a million Tesla cars on the road with Full Self-Driving hardware, feature complete, at a reliability level that we would consider that no one needs to pay attention, meaning you could go to sleep,” Musk said at Tesla’s 2019 Autonomy Day. “From our standpoint, if you fast forward a year, maybe a year and three months, but next year for sure, we will have over a million robotaxis on the road. The fleet wakes up with an over the air update. That’s all it takes.” That, of course, didn’t happen by 2020. Tesla’s Full Self-Driving software, which is on hundreds of thousands of vehicles today, relies only on cameras to perceive the environment around it. Industry experts say this vision-only approach is the reason why the software is still not actually fully self-driving, despite its name. FSD can perform many automated driving tasks, but still requires a human behind the wheel to stay attentive and take over if needed. It’s also not clear that existing Teslas even have the right hardware to get to this full self-driving future that Musk has been promising for years. As Musk posted on X in July, the roughly 5x increase in parameter count needed to power Tesla’s next-gen AI “is very difficult to achieve without upgrading the vehicle inference computer.” Regardless, if Tesla wants to commercialize Level 4 autonomous driving – which means the vehicle can drive itself under certain conditions without needing a human to take over – it will need to prove the safety case. Tesla has been under numerous federal investigations for fatal crashes that happened while Autopilot, Tesla’s lower level advanced driver assistance system, was in place. California has the most rigorous permitting process for testing and deployment of autonomous vehicles, but in most other states, Tesla would have to show at a minimum that its vehicles are capable of pulling themselves over safely. Then there’s the matter of the Cybercab’s lack of steering wheels or pedals, which would put it out of compliance with federal vehicle safety laws. GM’s Cruise had previously tried to bring its purpose-built robotaxi, the Origin, to production, but failed to gain the necessary approvals from the National Highway Traffic and Safety Administration before scrapping the project.              ',\n",
       "  'TitleContentVector': array([ 0.02240481, -0.0338538 ,  0.01056265, ..., -0.00512472,\n",
       "         -0.01647374, -0.0184863 ])},\n",
       " {'id': 6,\n",
       "  'title': 'Instagram blames some moderation issues on human reviewers, not AI',\n",
       "  'author': 'Sarah Perez',\n",
       "  'content': 'Instagram head Adam Mosseri on Friday addressed the moderation issues that saw Instagram and Threads users losing access to their accounts, having posts disappear, and more, saying that the company “found mistakes” that it’s attributing to human moderators. The company did not initially blame faulty AI systems, as many believed would be the case. In a post on Threads, Mosseri addressed the issue that has been plaguing the social platforms over the past several days, adding that the mistakes it’s found so far were due to content reviewers — people, not automated systems — “making calls without being provided the context on how conversations played out, which was a miss.” The exec said Instagram was fixing the issues so the reviewers could make better calls and make fewer mistakes. In a reply to a comment on the thread, he also clarified that, of course, Instagram knew that reviewers need context and that “one of the tools we built broke, and so it wasn’t showing them sufficient context.” “That’s on us,” he said. This explanation doesn’t seem to fully account for the range of issues that users were experiencing, as some found their accounts erroneously labeled as belonging to a user under the age of 13, then disabled. It’s unclear how a human moderator would have made this assumption. In addition, according to a report by The Verge, even after a user submitted their ID to verify their age, their account still remained disabled. Reached for comment, Instagram told TechCrunch that not all the issues Instagram users had encountered were related to human moderators and their mistakes. The company also pointed to the tool breakage and said the issues around things like flagging users as underage were still being investigated. Instagram doesn’t know that it ever will offer a detailed explanation for all the problems, we understand, as there may be several things at play. In addition to accounts being disabled, others had seen their posts being downranked or marked as spam, even when they were a reputable person or someone with a large following, not a spammer. For instance, former Wall Street Journal tech columnist Walt Mossberg remarked that his engagement on Threads had rapidly plummeted to zero. Instead of getting between 100 and 1,000 likes on his posts, that dropped to 0-20 within 24 hours, he said on Threads. Social media strategist Matt Navarra also pointed out that, in addition to having moderation problems himself, users were reporting that their follower growth rate and engagement was “falling off a cliff.” Bluesky, a social networking startup also competing with X (formerly Twitter) and Threads took advantage of the disruption on the site to drive frustrated users to its platform by creating an account on Threads and sharing its features and updates. “We’re trying to provide a safer experience, and we need to do better,” Mosseri said. He ended his post with a message that suggests the problem may not fully be resolved, adding, “Thanks for your patience and keep the feedback coming.” Updated, 11/11/24, 1:20 pm et with further comment from Instagram.         ',\n",
       "  'image_src': 'https://techcrunch.com/wp-content/uploads/2024/05/Instagram-Threads-GettyImages-1795093602.webp?w=1024',\n",
       "  'TitleContent': 'Instagram blames some moderation issues on human reviewers, not AIInstagram head Adam Mosseri on Friday addressed the moderation issues that saw Instagram and Threads users losing access to their accounts, having posts disappear, and more, saying that the company “found mistakes” that it’s attributing to human moderators. The company did not initially blame faulty AI systems, as many believed would be the case. In a post on Threads, Mosseri addressed the issue that has been plaguing the social platforms over the past several days, adding that the mistakes it’s found so far were due to content reviewers — people, not automated systems — “making calls without being provided the context on how conversations played out, which was a miss.” The exec said Instagram was fixing the issues so the reviewers could make better calls and make fewer mistakes. In a reply to a comment on the thread, he also clarified that, of course, Instagram knew that reviewers need context and that “one of the tools we built broke, and so it wasn’t showing them sufficient context.” “That’s on us,” he said. This explanation doesn’t seem to fully account for the range of issues that users were experiencing, as some found their accounts erroneously labeled as belonging to a user under the age of 13, then disabled. It’s unclear how a human moderator would have made this assumption. In addition, according to a report by The Verge, even after a user submitted their ID to verify their age, their account still remained disabled. Reached for comment, Instagram told TechCrunch that not all the issues Instagram users had encountered were related to human moderators and their mistakes. The company also pointed to the tool breakage and said the issues around things like flagging users as underage were still being investigated. Instagram doesn’t know that it ever will offer a detailed explanation for all the problems, we understand, as there may be several things at play. In addition to accounts being disabled, others had seen their posts being downranked or marked as spam, even when they were a reputable person or someone with a large following, not a spammer. For instance, former Wall Street Journal tech columnist Walt Mossberg remarked that his engagement on Threads had rapidly plummeted to zero. Instead of getting between 100 and 1,000 likes on his posts, that dropped to 0-20 within 24 hours, he said on Threads. Social media strategist Matt Navarra also pointed out that, in addition to having moderation problems himself, users were reporting that their follower growth rate and engagement was “falling off a cliff.” Bluesky, a social networking startup also competing with X (formerly Twitter) and Threads took advantage of the disruption on the site to drive frustrated users to its platform by creating an account on Threads and sharing its features and updates. “We’re trying to provide a safer experience, and we need to do better,” Mosseri said. He ended his post with a message that suggests the problem may not fully be resolved, adding, “Thanks for your patience and keep the feedback coming.” Updated, 11/11/24, 1:20 pm et with further comment from Instagram.         ',\n",
       "  'TitleContentVector': array([-0.02223641, -0.01155761, -0.0073034 , ..., -0.01038587,\n",
       "          0.00076895, -0.0360576 ])}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = embedding_df.to_dict(\"records\")\n",
    "docs[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    try:\n",
    "        if \"id\" in doc:\n",
    "            es.index(index=\"news_content_14\", document=doc, id=doc[\"id\"])\n",
    "        else:\n",
    "            print(f\"ID missing in document: {doc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing document with ID {doc.get('id', 'Unknown')}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 6, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.count(index=\"news_content_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keyword = \"facebook attributes certain filtering problems on human judges rather than artificial intelligence.\"\n",
    "vector_of_input_keyword = get_embedding(input_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/fgqb1nkn4qx_2lsc6qjgqwfm0000gn/T/ipykernel_3131/2520431747.py:8: ElasticsearchWarning: The kNN search API has been replaced by the `knn` option in the search API.\n",
      "  res = es.knn_search(index=\"news_content_14\", knn=query , source=[\"title\",\"author\",\"content\",\"image_src\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_index': 'news_content_14',\n",
       "  '_id': '6',\n",
       "  '_score': 0.75510013,\n",
       "  '_source': {'title': 'Instagram blames some moderation issues on human reviewers, not AI',\n",
       "   'author': 'Sarah Perez',\n",
       "   'content': 'Instagram head Adam Mosseri on Friday addressed the moderation issues that saw Instagram and Threads users losing access to their accounts, having posts disappear, and more, saying that the company “found mistakes” that it’s attributing to human moderators. The company did not initially blame faulty AI systems, as many believed would be the case. In a post on Threads, Mosseri addressed the issue that has been plaguing the social platforms over the past several days, adding that the mistakes it’s found so far were due to content reviewers — people, not automated systems — “making calls without being provided the context on how conversations played out, which was a miss.” The exec said Instagram was fixing the issues so the reviewers could make better calls and make fewer mistakes. In a reply to a comment on the thread, he also clarified that, of course, Instagram knew that reviewers need context and that “one of the tools we built broke, and so it wasn’t showing them sufficient context.” “That’s on us,” he said. This explanation doesn’t seem to fully account for the range of issues that users were experiencing, as some found their accounts erroneously labeled as belonging to a user under the age of 13, then disabled. It’s unclear how a human moderator would have made this assumption. In addition, according to a report by The Verge, even after a user submitted their ID to verify their age, their account still remained disabled. Reached for comment, Instagram told TechCrunch that not all the issues Instagram users had encountered were related to human moderators and their mistakes. The company also pointed to the tool breakage and said the issues around things like flagging users as underage were still being investigated. Instagram doesn’t know that it ever will offer a detailed explanation for all the problems, we understand, as there may be several things at play. In addition to accounts being disabled, others had seen their posts being downranked or marked as spam, even when they were a reputable person or someone with a large following, not a spammer. For instance, former Wall Street Journal tech columnist Walt Mossberg remarked that his engagement on Threads had rapidly plummeted to zero. Instead of getting between 100 and 1,000 likes on his posts, that dropped to 0-20 within 24 hours, he said on Threads. Social media strategist Matt Navarra also pointed out that, in addition to having moderation problems himself, users were reporting that their follower growth rate and engagement was “falling off a cliff.” Bluesky, a social networking startup also competing with X (formerly Twitter) and Threads took advantage of the disruption on the site to drive frustrated users to its platform by creating an account on Threads and sharing its features and updates. “We’re trying to provide a safer experience, and we need to do better,” Mosseri said. He ended his post with a message that suggests the problem may not fully be resolved, adding, “Thanks for your patience and keep the feedback coming.” Updated, 11/11/24, 1:20 pm et with further comment from Instagram.         ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2024/05/Instagram-Threads-GettyImages-1795093602.webp?w=1024'}},\n",
       " {'_index': 'news_content_14',\n",
       "  '_id': '2',\n",
       "  '_score': 0.6983246,\n",
       "  '_source': {'title': 'Researchers question AI’s ‘reasoning’ ability as models stumble on math problems with trivial changes',\n",
       "   'author': 'Devin Coldewey',\n",
       "   'content': 'How do machine learning models do what they do? And are they really “thinking” or “reasoning” the way we understand those things? This is a philosophical question as much as a practical one, but a new paper making the rounds Friday suggests that the answer is, at least for now, a pretty clear “no.” A group of AI research scientists at Apple released their paper, “Understanding the limitations of mathematical reasoning in large language models,” to general commentary Thursday. While the deeper concepts of symbolic learning and pattern reproduction are a bit in the weeds, the basic concept of their research is very easy to grasp. Let’s say I asked you to solve a simple math problem like this one: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday. How many kiwis does Oliver have? Obviously, the answer is 44 + 58 + (44 * 2) = 190. Though large language models are actually spotty on arithmetic, they can pretty reliably solve something like this. But what if I threw in a little random extra info, like this: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Saturday. On Sunday, he picks double the number of kiwis he did on Friday, but five of them were a bit smaller than average. How many kiwis does Oliver have? It’s the same math problem, right? And of course even a grade-schooler would know that even a small kiwi is still a kiwi. But as it turns out, this extra data point confuses even state-of-the-art LLMs. Here’s GPT-o1-mini’s take: … on Sunday, 5 of these kiwis were smaller than average. We need to subtract them from the Sunday total: 88 (Sunday’s kiwis) – 5 (smaller kiwis) = 83 kiwis This is just a simple example out of hundreds of questions that the researchers lightly modified, but nearly all of which led to enormous drops in success rates for the models attempting them. Now, why should this be? Why would a model that understands the problem be thrown off so easily by a random, irrelevant detail? The researchers propose that this reliable mode of failure means the models don’t really understand the problem at all. Their training data does allow them to respond with the correct answer in some situations, but as soon as the slightest actual “reasoning” is required, such as whether to count small kiwis, they start producing weird, unintuitive results. As the researchers put it in their paper: [W]e investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. This observation is consistent with the other qualities often attributed to LLMs due to their facility with language. When, statistically, the phrase “I love you” is followed by “I love you, too,” the LLM can easily repeat that — but it doesn’t mean it loves you. And although it can follow complex chains of reasoning it has been exposed to before, the fact that this chain can be broken by even superficial deviations suggests that it doesn’t actually reason so much as replicate patterns it has observed in its training data. Mehrdad Farajtabar, one of the co-authors, breaks down the paper very nicely in this thread on X. An OpenAI researcher, while commending Mirzadeh et al’s work, objected to their conclusions, saying that correct results could likely be achieved in all these failure cases with a bit of prompt engineering. Farajtabar (responding with the typical yet admirable friendliness researchers tend to employ) noted that while better prompting may work for simple deviations, the model may require exponentially more contextual data in order to counter complex distractions — ones that, again, a child could trivially point out. Does this mean that LLMs don’t reason? Maybe. That they can’t reason? No one knows. These are not well-defined concepts, and the questions tend to appear at the bleeding edge of AI research, where the state of the art changes on a daily basis. Perhaps LLMs “reason,” but in a way we don’t yet recognize or know how to control. It makes for a fascinating frontier in research, but it’s also a cautionary tale when it comes to how AI is being sold. Can it really do the things they claim, and if it does, how? As AI becomes an everyday software tool, this kind of question is no longer academic.         ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-496822526.jpg?w=1024'}},\n",
       " {'_index': 'news_content_14',\n",
       "  '_id': '3',\n",
       "  '_score': 0.6963611,\n",
       "  '_source': {'title': 'Silicon Valley is debating if AI weapons should be allowed to decide to kill',\n",
       "   'author': 'Margaux MacColl',\n",
       "   'content': 'In late September, Shield AI co-founder Brandon Tseng swore that weapons in the U.S. would never be fully autonomous — meaning an AI algorithm would make the final decision to kill someone. “Congress doesn’t want that,” the defense tech founder told TechCrunch. “No one wants that.” But Tseng spoke too soon. Five days later, Anduril co-founder Palmer Luckey expressed an openness to autonomous weapons — or at least a heavy skepticism of arguments against them. The U.S.’s adversaries “use phrases that sound really good in a sound bite: Well, can’t you agree that a robot should never be able to decide who lives and dies?” Luckey said during a talk earlier this month at Pepperdine University. “And my point to them is, where’s the moral high ground in a landmine that can’t tell the difference between a school bus full of kids and a Russian tank?” When asked for further comment, Shannon Prior, a spokesperson for Anduril, said that Luckey didn’t mean that robots should be programmed to kill people on their own, just that he was concerned about “bad people using bad AI.” In the past, Silicon Valley has erred on the side of caution. Take it from Luckey’s co-founder, Trae Stephens. “I think the technologies that we’re building are making it possible for humans to make the right decisions about these things,” he told Kara Swisher last year. “So that there is an accountable, responsible party in the loop for all decisions that could involve lethality, obviously.” The Anduril spokesperson denied any dissonance between Luckey (pictured above) and Stephens’ perspectives, and said that Stephens didn’t mean that a human should always make the call, but just that someone is accountable. To be fair, the stance of the U.S. government itself is similarly ambiguous. The U.S. military currently does not purchase fully autonomous weapons. Though some argue weapons like mines and missiles can operate autonomously, this is a qualitatively different form of autonomy than, say, a turret that identifies, acquires, and fires on targets without human intervention. The U.S. does not ban companies from making fully autonomous lethal weapons nor does it explicitly ban them from selling such things to foreign countries. Last year, the U.S. released updated guidelines for AI safety in the military that have been endorsed by many U.S. allies and requires top military officials to approve of any new autonomous weapon; yet the guidelines are voluntary (Anduril said it is committed to following the guidelines), and U.S. officials have continuously said it’s “not the right time” to consider any binding ban on autonomous weapons. Last month, Palantir co-founder and Anduril investor Joe Lonsdale also showed a willingness to consider fully autonomous weapons. At an event hosted by the think tank Hudson Institute, Lonsdale expressed frustration that this question is being framed as a yes-or-no at all. He instead presented a hypothetical where China has embraced AI weapons, but the U.S. has to “press the button every time it fires.” He encouraged policymakers to embrace a more flexible approach to how much AI is in weapons. “You very quickly realize, well, my assumptions were wrong if I just put a stupid top-down rule, because I’m a staffer who’s never played this game before,” he said. “I could destroy us in the battle.” When TechCrunch asked Lonsdale for further comment, he emphasized that defense tech companies shouldn’t be the ones setting the agenda on lethal AI. “The key context to what I was saying is that our companies don’t make the policy, and don’t want to make the policy: it’s the job of elected officials to make the policy,” he said. “But they do need to educate themselves on the nuance to do a good job.” He also reiterated a willingness to consider more autonomy in weapons. “It’s not a binary as you suggest — ‘fully autonomous or not’ isn’t the correct policy question. There’s a sophisticated dial along a few different dimensions for what you might have a soldier do and what you have the weapons system do,” he said. “Before policymakers put these rules in place and decide where the dials need to be set in what circumstance, they need to learn the game and learn what the bad guys might be doing, and what’s necessary to win with American lives on the line.” Activists and human rights groups have long tried and failed to establish international bans on autonomous lethal weapons — bans that the U.S. has resisted signing. But the war in Ukraine may have turned the tide against activists, providing both a trove of combat data and a battlefield for defense tech founders to test on. Currently, companies integrate AI into weapons systems, although they still require a human to make the final decision to kill. Meanwhile, Ukrainian officials have pushed for more automation in weapons, hoping it’ll give them a leg-up over Russia. “We need maximum automation,” said Mykhailo Fedorov, Ukraine’s minister of digital transformation, in an interview with The New York Times. “These technologies are fundamental to our victory.” For many in Silicon Valley and D.C., the biggest fear is that China or Russia rolls out fully autonomous weapons first, forcing the U.S.’s hand. At a UN debate on AI arms last year, a Russian diplomat was notably coy. “We understand that for many delegations the priority is human control,” he said. “For the Russian Federation, the priorities are somewhat different.” At the Hudson Institute event, Lonsdale said that the tech sector needs to take it upon itself to “teach the Navy, teach the DoD, teach Congress” about the potential of AI to “hopefully get us ahead of China.” Lonsdale’s and Luckey’s affiliated companies are working on getting Congress to listen to them. Anduril and Palantir have cumulatively spent over $4 million in lobbying this year, according to OpenSecrets. Editor’s note: this story was updated with more language to describe autonomous weapons.         ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2023/12/GettyImages-1258953220-1.jpg?w=1024'}},\n",
       " {'_index': 'news_content_14',\n",
       "  '_id': '1',\n",
       "  '_score': 0.68221843,\n",
       "  '_source': {'title': 'Anthropic CEO goes full techno-optimist in 15,000-word paean to AI',\n",
       "   'author': 'Kyle Wiggers',\n",
       "   'content': 'Anthropic CEO Dario Amodei wants you to know he’s not an AI “doomer.” At least, that’s my read of the “mic drop” of a ~15,000 word essay Amodei published to his blog late Friday. (I tried asking Anthropic’s Claude chatbot whether it concurred, but alas, the post exceeded the free plan’s length limit.) In broad strokes, Amodei paints a picture of a world in which all AI risks are mitigated, and the tech delivers heretofore unrealized prosperity, social uplift, and abundance. He asserts this isn’t to minimize AI’s downsides — at the start, Amodei takes aim, without naming names, at AI companies overselling and generally propagandizing their tech’s capabilities. But one might argue that the essay leans too far in the techno-utopianist direction, making claims simply unsupported by fact. Amodei believes that “powerful AI” will arrive as soon as 2026. By powerful AI, he means AI that’s “smarter than a Nobel Prize winner” in fields like biology and engineering, and that can perform tasks like proving unsolved mathematical theorems and writing “extremely good novels.” This AI, Amodei says, will be able to control any software or hardware imaginable, including industrial machinery, and essentially do most jobs humans do today — but better. “[This AI] can engage in any actions, communications, or remote operations … including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on,” Amodei writes. “It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory it could even design robots or equipment for itself to use.” Lots would have to happen to reach that point. Even the best AI today can’t “think” in the way we understand it. Models don’t so much reason as replicate patterns they’ve observed in their training data. Assuming for the purpose of Amodei’s argument that the AI industry does soon “solve” human-like thought, would robotics catch up to allow future AI to perform lab experiments, manufacture its own tools, and so on? The brittleness of today’s robots imply it’s a long shot. Yet Amodei is optimistic — very optimistic. He believes AI could, in the next 7-12 years, help treat nearly all infectious diseases, eliminate most cancers, cure genetic disorders, and halt Alzheimer’s at the earliest stages. In the next 5-10 years, Amodei thinks that conditions like PTSD, depression, schizophrenia, and addiction will be cured with AI-concocted drugs, or genetically prevented via embryo screening (a controversial opinion) — and that AI-developed drugs will also exist that “tune cognitive function and emotional state” to “get [our brains] to behave a bit better and have a more fulfilling day-to-day experience.” Should this come to pass, Amodei expects the average human lifespan to double to 150. “My basic prediction is that AI-enabled biology and medicine will allow us to compress the progress that human biologists would have achieved over the next 50-100 years into 5-10 years,” he writes. “I’ll refer to this as the ‘compressed 21st century’: the idea that after powerful AI is developed, we will in a few years make all the progress in biology and medicine that we would have made in the whole 21st century.” These seem like stretches, too, considering that AI hasn’t radically transformed medicine yet — and may not for quite some time, or ever. Even if AI does reduce the labor and cost involved in getting a drug into pre-clinical testing, it may fail at a later stage, just like human-designed drugs. Consider that the AI deployed in healthcare today has been shown to be biased and risky in a number of ways, or otherwise incredibly difficult to implement in existing clinical and lab settings. Suggesting all these issues and more will be solved roughly within the decade seems, well, aspirational. But Amodei doesn’t stop there. AI could solve world hunger, he claims. It could turn the tide on climate change. And it could transform the economies in most developing countries; Amodei believes AI can bring the per-capita GDP of sub-Saharan Africa ($1,701 as of 2022) to the per-capita GDP of China ($12,720 in 2022) in 5-10 years. These are bold pronouncements, although likely familiar to anyone who’s listened to disciples of the “Singularity” movement, which expects similar results. To Amodei’s credit, he acknowledges that such developments would require “a huge effort in global health, philanthropy, [and] political advocacy,” which he posits will occur because it’s in the world’s best economic interest. That would be a dramatic change in human behavior if so, given people have shown time and again that their primary interest is in what benefits them in the shorter term. (Deforestation is but one example among thousands.) It’s also worth noting that many of the workers responsible for labeling the datasets used to train AI are paid far below minimum wage while their employers reap tens of millions — or hundreds of millions — in capital from the results. Amodei touches, briefly, on the dangers of AI to civil society, proposing that a coalition of democracies secure AI’s supply chain and block adversaries who intend to use AI toward harmful ends from the means of powerful AI production (semiconductors, etc.). In the same breath, he suggests that AI, in the right hands, could be used to “undermine repressive governments” and even reduce bias in the legal system. (AI has historically exacerbated biases in the legal system.) “A truly mature and successful implementation of AI has the potential to reduce bias and be fairer for everyone,” Amodei writes. So, if AI takes over every conceivable job and does it better and faster, won’t that leave humans in a lurch economically speaking? Amodei admits that, yes, it would, and that at that point, society would have to have conversations about “how the economy should be organized.” But he offers no solution. “People do want a sense of accomplishment, even a sense of competition, and in a post-AI world it will be perfectly possible to spend years attempting some very difficult task with a complex strategy, similar to what people do today when they embark on research projects, try to become Hollywood actors, or found companies,” he writes. “The facts that (a) an AI somewhere could in principle do this task better, and (b) this task is no longer an economically rewarded element of a global economy, don’t seem to me to matter very much.” Amodei advances the notion, in wrapping up, that AI is simply a technological accelerator — that humans naturally trend toward “rule of law, democracy, and Enlightenment values.” But in doing so, he ignores AI’s many costs. AI is projected to have — is already having — an enormous environmental impact. And it’s creating inequality. Nobel Prize-winning economist Joseph Stiglitz and others have noted the labor disruptions caused by AI could further concentrate wealth in the hands of companies and leave workers more powerless than ever. These companies include Anthropic, as loath as Amodei is to admit it. Anthropic is a business, after all — one reportedly worth close to $40 billion. And those benefiting from its AI tech are, by and large, corporations whose only responsibility is to boost returns to shareholders, not better humanity. A cynic might question the essay’s timing, in fact, given that Anthropic is said to be in the process of raising billions of dollars in venture funds. OpenAI CEO Sam Altman published a similarly technopotimist manifesto shortly before OpenAI closed a $6.5 billion funding round. Perhaps it’s a coincidence. Then again, Amodei isn’t a philanthropist. Like any CEO, he has a product to pitch. It just so happens that his product is going to “save the world” — and those who think otherwise risk being left behind. Or so he’d have you believe.             ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2023/09/53202070940_ea57312b1a_k.jpg?w=1024'}},\n",
       " {'_index': 'news_content_14',\n",
       "  '_id': '5',\n",
       "  '_score': 0.6273291,\n",
       "  '_source': {'title': 'Tesla reveals 20 Cybercabs at We, Robot event, says you’ll be able to buy one for less than $30,000',\n",
       "   'author': 'Rebecca Bellan',\n",
       "   'content': 'Tesla has finally revealed its Cybercab, and it looks like a smaller, sleeker, two-seater Cybertruck. And while many were expecting there to be at least one prototype of a robotaxi with no steering wheel or pedals, Tesla CEO Elon Musk delighted his fans with a lineup of 20 vehicles. The flashy “We, Robot” event took place at Warner Bros. Discovery studio on Thursday. Before walking on stage, Musk walked over to a robotaxi, which opened its gullwing doors, and did a short demo around the well-maintained streets of the Hollywood studio. Musk repeated previous claims that the cost of autonomous transport will be so low, it will be akin to “individualized mass transit.” He said he believed the average operating cost of the Cybercab will be over time around $0.20 per mile. “And you will be able to buy one,” Musk said, adding that the cost of the vehicle would be below $30,000. Musk also noted he expects Tesla to start doing “unsupervised FSD in Texas and California next year” with the Model 3 and Model Y. He acknowledged that he’s too optimistic about timelines, but said he expects the Cybercab to be in production by 2026 or “before 2027.” Fans cheered when Musk said they would be able to test out the Cybercabs themselves at the event. “They have like 20 of them driving around the entire lot totally unsupervised,” one event-goer told TechCrunch. The robotaxi also doesn’t have a plug in charger, and instead has “inductive charging,” which is a sort of wireless charging, according to Musk. Tesla unveiled a surprise Robovan at the event, a sleek-looking autonomous bus that can carry up to 20 people and also transport goods. No timelines were revealed for this vehicle — only a hand-waving of a future that could “change the look of the roads.” Musk also introduced roughly a dozen Optimus humanoid robots, sharing his vision for a future where robots act as friend and helper for the low price of around $20,000 to $30,000. Those bots were walking among the humans at the event, dancing like go-go dancers, and even mixing drinks. They also appeared to be speaking to the guests, and could do different accents and voice personas on command. Though it’s not clear whether those capabilities, and the bots’ movements, were being remote controlled by a human. Tesla was originally slated to reveal its Robotaxi or Cybercab in August, but delayed the unveiling after the executive requested an “important design change to the front.” The Robotaxi unveiling is part of Tesla’s push to go “balls to the wall for autonomy” this year after pivoting from prioritizing the production of a $25,000 EV and laying off 10% of staff, including most of the charging team. But Musk’s vision of an autonomous driving future has been in play for years, and a large part of the reason why investors price Tesla’s stock not as an automaker, but as a technology company. The Cybercab prototype represents one half of the business concept Musk has set forth since at least 2019, wherein the automaker would run its own fleet of dedicated robotaxis on a Tesla ride-hail app, which Tesla teased during its first quarter investor call. Musk has described the other half of the strategy as similar to Uber or Airbnb, where Tesla owners will be able to add their properly equipped vehicles to Tesla’s ride-hailing app to make extra money when the cars are not in use, and Tesla will take 25% to 30% of the revenue (similar to Apple’s App Store take rate). Musk also said that Tesla projected that robotaxi rides would cost less than public transportation, but he didn’t say by when. “By the middle of next year, we’ll have over a million Tesla cars on the road with Full Self-Driving hardware, feature complete, at a reliability level that we would consider that no one needs to pay attention, meaning you could go to sleep,” Musk said at Tesla’s 2019 Autonomy Day. “From our standpoint, if you fast forward a year, maybe a year and three months, but next year for sure, we will have over a million robotaxis on the road. The fleet wakes up with an over the air update. That’s all it takes.” That, of course, didn’t happen by 2020. Tesla’s Full Self-Driving software, which is on hundreds of thousands of vehicles today, relies only on cameras to perceive the environment around it. Industry experts say this vision-only approach is the reason why the software is still not actually fully self-driving, despite its name. FSD can perform many automated driving tasks, but still requires a human behind the wheel to stay attentive and take over if needed. It’s also not clear that existing Teslas even have the right hardware to get to this full self-driving future that Musk has been promising for years. As Musk posted on X in July, the roughly 5x increase in parameter count needed to power Tesla’s next-gen AI “is very difficult to achieve without upgrading the vehicle inference computer.” Regardless, if Tesla wants to commercialize Level 4 autonomous driving – which means the vehicle can drive itself under certain conditions without needing a human to take over – it will need to prove the safety case. Tesla has been under numerous federal investigations for fatal crashes that happened while Autopilot, Tesla’s lower level advanced driver assistance system, was in place. California has the most rigorous permitting process for testing and deployment of autonomous vehicles, but in most other states, Tesla would have to show at a minimum that its vehicles are capable of pulling themselves over safely. Then there’s the matter of the Cybercab’s lack of steering wheels or pedals, which would put it out of compliance with federal vehicle safety laws. GM’s Cruise had previously tried to bring its purpose-built robotaxi, the Origin, to production, but failed to gain the necessary approvals from the National Highway Traffic and Safety Administration before scrapping the project.              ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2024/10/cybercab.png?w=1024'}},\n",
       " {'_index': 'news_content_14',\n",
       "  '_id': '4',\n",
       "  '_score': 0.62199897,\n",
       "  '_source': {'title': 'Elon Musk unveils the Robovan: the biggest surprise from Tesla’s We, Robot event',\n",
       "   'author': 'Rebecca Bellan',\n",
       "   'content': 'Elon Musk unveiled a prototype of Tesla’s Robovan on Thursday night during the company’s We, Robot event in Los Angeles. The Robovan will be an electric, autonomous vehicle roughly the size of a bus, designed for transporting people around high density areas. It will carry up to 20 people at a time and also transport goods, according to Musk. “We’re going to make this, and it’s going to look like that,” said Musk on Thursday night as the Robovan rolled towards center stage. That’s about as much as Musk was willing to say, and we’re not even sure that much is true. Musk didn’t mention how much the Robovan would cost, how Tesla would produce it, or when it will come out. However, it does look pretty cool. The Robovan has a retro-futuristic look – somewhere between a bus from The Jetsons and a toaster from the 1950s. It features silver metallic sides with black details, and strips of light running parallel to the ground along its sides, with doors that slide out from the middle. Inside, there are seats and room to stand, with tinted windows throughout. There is no steering wheel, since it’s autonomous. “One of the things we want to do – and we’ve done this with the Cybertruck – is we want to change the look of the roads,” said Musk. “The future should look like the future,” he said, repeating an old line. It looks similar to other purpose-built robotaxis, like those designed by Zoox and Cruise. Only Tesla’s van is much bigger. In China, WeRide has built a similar Robobus. That said, the Robovan showed on Thursday is only a prototype. Despite what Musk says, there’s no telling what the real thing will look like or when it will actually come out. Tesla had kept the design of the vehicles it introduced on Thursday pretty close to the chest. The only real hint we had was from Tesla’s 2023 Investor Day, when the automaker teased a couple of new vehicles that appeared to be designed for volume production: One smaller vehicle that appears now to be the Cybercab, and a larger one that we can now say is likely the Robovan. The stated goal at the time was to produce 20 million vehicles per year by 2030. That would mean that Tesla needs to increase production and sales by about 15 times from 2022. During Thursday’s event, Musk did not outline any plans for building new production facilities or retooling existing facilities to accommodate either the Cybercab or the Robovan. He also didn’t provide much in the way of timelines for the Robovan, though he predicted the Cybercab would start production in 2026 or 2027.     ',\n",
       "   'image_src': 'https://techcrunch.com/wp-content/uploads/2024/10/Screenshot-2024-10-10-at-9.09.45PM.png?w=1024'}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"field\" : \"TitleContentVector\",\n",
    "    \"query_vector\" : vector_of_input_keyword,\n",
    "    \"k\" : 10,\n",
    "    \"num_candidates\" : 500, \n",
    "}\n",
    "\n",
    "res = es.knn_search(index=\"news_content_14\", knn=query , source=[\"title\",\"author\",\"content\",\"image_src\"])\n",
    "res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = {\n",
    "    \"knn\": {\n",
    "        \"field\": \"TitleContentVector\",\n",
    "        \"query_vector\": vector_of_input_keyword,\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 10000\n",
    "    },\n",
    "    \"_source\": [\"ProductName\",\"Description\",\"PrimaryColor\",\"Price (INR)\",\"ProductBrand\",\"Gender\"]\n",
    "}\n",
    "\n",
    "# Add Price greater than and less than filters\n",
    "min_price = 0\n",
    "max_price = 2000\n",
    "Gender = \"Men\"\n",
    "\n",
    "filter_query = {\n",
    "    \"bool\": {\n",
    "        \"must\": [\n",
    "            {\n",
    "                \"match\": {\n",
    "                    \"Gender\": {\n",
    "                        \"query\": Gender,\n",
    "                        \"fuzzy_transpositions\": \"false\",\n",
    "                        \"fuzziness\": 0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"range\": {\n",
    "                    \"Price (INR)\": {\n",
    "                        \"gte\": min_price,\n",
    "                        \"lte\": max_price\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "res = es.knn_search(index=\"news_content_12\",  # change index name here.\n",
    "                    body=q1,\n",
    "                    request_timeout=5000,\n",
    "                    filter=filter_query)\n",
    "\n",
    "res[\"hits\"][\"hits\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
